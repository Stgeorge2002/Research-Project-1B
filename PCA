import subprocess
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# List of required packages
required_packages = ["numpy", "pandas", "scikit-learn", "matplotlib"]

# Attempt to import required packages, install if they're missing
for package in required_packages:
    try:
        __import__(package)
    except ImportError:
        install(package)

# Load the dataset
data = pd.read_csv('C:/Users/theoa/OneDrive/Desktop/Bath/Research Project 1B/test/AD.csv')  # Update with your file path

# Select only the columns from the 6th column onward
features = data.columns[5:]  # Adjust index if needed
x = data.loc[:, features].values

# Standardizing the features
x = StandardScaler().fit_transform(x)

# Applying PCA
pca = PCA(n_components=20)
principalComponents = pca.fit_transform(x)

# Identifying top 15 features for each of the first 20 components
top_features_per_component = {}
for i in range(20):
    component = pca.components_[i]
    sorted_features = sorted(zip(features, component), key=lambda x: np.abs(x[1]), reverse=True)
    top_features_per_component[f'PC{i+1}'] = [feature[0] for feature in sorted_features[:15]]

# Creating a DataFrame from top_features_per_component
top_features_df = pd.DataFrame(top_features_per_component)

# Tallying the occurrences of each feature
feature_tally = top_features_df.apply(pd.Series.value_counts).fillna(0).sum(axis=1).astype(int)

# Filtering out features with fewer than 4 occurrences
selected_features = feature_tally[feature_tally >= 4].index.tolist()

# Filter the original dataset to keep only the selected features
# Add back any columns prior to the 6th column if they are important (e.g., ID, labels)
important_columns = data.columns[:5].tolist() + selected_features
filtered_data = data[important_columns]

# Save the filtered dataset or continue with further analysis
filtered_data.to_csv('filtered_dataset.csv', index=False)  # Saving the filtered dataset
print(filtered_data.head())  # Displaying the first few rows of the filtered dataset
